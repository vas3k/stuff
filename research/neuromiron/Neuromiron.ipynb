{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейромирон: попытка сделать style transfer для текстов\n",
    "\n",
    "Посты в блоге с описаниями здесь: http://vas3k.ru/blog/394/ и здесь http://vas3k.ru/blog/393/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import operator\n",
    "import random\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from pyphonetics import Soundex\n",
    "from transliterate import translit\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "\n",
    "STYLE_FILENAME = \"./oxxxy.txt\"\n",
    "HAWKING_CONTENT_FILENAME = \"./hawking.txt\"\n",
    "UK_CONTENT_FILENAME = \"./uk.txt\"\n",
    "WORD_RE = re.compile(r'[а-яА-ЯёЁa-zA-Z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/Users/vas3k/Dev/neuromiron/poet-ex-machina\")  \n",
    "# ^ эту фигню надо скачать с гитхаба и установить, но у меня не собиралась, потому вот так\n",
    "\n",
    "import includes.accentsandsyllables as accentsandsyllables\n",
    "from includes.utils import Utils\n",
    "from includes.rhymesandritms import RhymesAndRitms\n",
    "accents = accentsandsyllables.AccentsAndSyllables()\n",
    "soundex = Soundex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пошли всякие вспомогательные функции\n",
    "\n",
    "### Эта чтобы не пересчитывать постоянно обработанные корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(*args):\n",
    "    for varname in args:\n",
    "        with open(\"{}.txt\".format(varname), \"r\") as f:\n",
    "            yield json.loads(f.read())\n",
    "            \n",
    "def save(**kwargs):\n",
    "    for varname, data in kwargs.items():\n",
    "        with open(\"{}.txt\".format(varname), \"w\") as f:\n",
    "            f.write(json.dumps(data))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эти разбирают слова на элементы и создают разные коды, с ними можно играться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_accent_soundex(word):\n",
    "    if len(word) > 1:\n",
    "        sdx = soundex.phonetics(translit(word, \"ru\", reversed=True))\n",
    "    else:\n",
    "        sdx = translit(word, \"ru\", reversed=True)\n",
    "        \n",
    "    num_syllables, accent_syllable = accents.getAccentsAndSyllablesWord(word)\n",
    "    rel_accent = accent_syllable / num_syllables\n",
    "    if rel_accent <= 0.4:\n",
    "        accent = 0\n",
    "    elif rel_accent <= 0.67:\n",
    "        accent = 1\n",
    "    else:\n",
    "        accent = 2\n",
    "        \n",
    "    _, pos = nltk.pos_tag([word], lang=\"rus\")[0]\n",
    "    real_pos = pos\n",
    "    if pos in (\"NONLEX\"):\n",
    "        return word\n",
    "    return \"{}{}{}\".format(pos[0], accent, sdx)\n",
    "\n",
    "print(pos_accent_soundex(\"неваляшка\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_soundex(word):\n",
    "    if word == \"/\":\n",
    "        return \"/\"\n",
    "    try:\n",
    "        if len(word) > 1:\n",
    "            sdx = soundex.phonetics(translit(word, \"ru\", reversed=True))\n",
    "        else:\n",
    "            sdx = translit(word, \"ru\", reversed=True)\n",
    "    except:\n",
    "        return \"\"\n",
    "        \n",
    "    _, pos = nltk.pos_tag([word], lang=\"rus\")[0]\n",
    "    if pos in (\"NONLEX\"):\n",
    "        return \"X{}\".format(sdx)\n",
    "    return \"{}{}\".format(pos[0], sdx)\n",
    "\n",
    "print(pos_soundex(\"неваляшка\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def syllables_soundex(word):\n",
    "    if word == \"/\":\n",
    "        return \"/\"\n",
    "    \n",
    "    try:\n",
    "        if len(word) > 1:\n",
    "            sdx = soundex.phonetics(translit(word, \"ru\", reversed=True))\n",
    "        else:\n",
    "            sdx = translit(word, \"ru\", reversed=True)\n",
    "    except:\n",
    "        return \"\"\n",
    "        \n",
    "    syllables = len(Utils.getWordSyllables(word))\n",
    "    return \"{}{}\".format(syllables, sdx)\n",
    "\n",
    "print(syllables_soundex(\"неваляшка\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_syllables_soundex(word):\n",
    "    if word == \"/\":\n",
    "        return \"/\"\n",
    "    \n",
    "    try:\n",
    "        if len(word) > 1:\n",
    "            sdx = soundex.phonetics(translit(word, \"ru\", reversed=True))\n",
    "        else:\n",
    "            sdx = translit(word, \"ru\", reversed=True)\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "    syllables = len(Utils.getWordSyllables(word))\n",
    "    \n",
    "    _, pos = nltk.pos_tag([word], lang=\"rus\")[0]\n",
    "    if pos in (\"NONLEX\"):\n",
    "        return \"X{}{}\".format(syllables, sdx)\n",
    "        \n",
    "    return \"{}{}{}\".format(pos[0], syllables, sdx)\n",
    "\n",
    "print(pos_syllables_soundex(\"неваляшка\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accent_soundex(word):\n",
    "    if word == \"/\":\n",
    "        return \"/\"\n",
    "    \n",
    "    if len(word) > 1:\n",
    "        sdx = soundex.phonetics(translit(word, \"ru\", reversed=True))\n",
    "    else:\n",
    "        sdx = translit(word, \"ru\", reversed=True)\n",
    "        \n",
    "    num_syllables, accent_syllable = accent_info(word)\n",
    "    rel_accent = accent_syllable / num_syllables\n",
    "    if rel_accent <= 0.4:\n",
    "        accent = 0\n",
    "    elif rel_accent <= 0.67:\n",
    "        accent = 1\n",
    "    else:\n",
    "        accent = 2\n",
    "        \n",
    "    return \"{}{}\".format(accent, sdx)\n",
    "\n",
    "print(accent_soundex(\"неваляшка\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def levenshtein_distance(word1, word2):\n",
    "    if len(word1) < len(word2):\n",
    "        return levenshtein_distance(word2, word1)\n",
    "\n",
    "    if len(word2) == 0:\n",
    "        return len(word1)\n",
    "\n",
    "    previous_row = list(range(len(word2) + 1))\n",
    "\n",
    "    for i, char1 in enumerate(word1):\n",
    "        current_row = [i + 1]\n",
    "\n",
    "        for j, char2 in enumerate(word2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (char1 != char2)\n",
    "\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_rhyme(word, rhymed_word):\n",
    "    word_num_syllables, word_accent_syllable = accents.getAccentsAndSyllablesWord(word)\n",
    "    rhymed_word_num_syllables, rhymed_word_accent_syllable = accents.getAccentsAndSyllablesWord(rhymed_word)\n",
    "    \n",
    "    rhymed_word_accented_syll_offset = rhymed_word_num_syllables - rhymed_word_accent_syllable\n",
    "    rhymed_end_1 = RhymesAndRitms.getRhymedEnd(rhymed_word, rhymed_word_accented_syll_offset)\n",
    "    word_accented_syll_offset = word_num_syllables - word_accent_syllable\n",
    "    rhymed_end_2 = RhymesAndRitms.getRhymedEnd(word, word_accented_syll_offset)\n",
    "    \n",
    "    if not rhymed_end_1 or not rhymed_end_2:\n",
    "        return False\n",
    "    \n",
    "#     if len(rhymed_end_1) != len(rhymed_end_2):\n",
    "#         return False\n",
    "    \n",
    "    j = len(rhymed_end_2) - 1\n",
    "    for i in range(len(rhymed_end_1) - 1, -1, -1):\n",
    "        if j < 0:\n",
    "            return True\n",
    "\n",
    "        c1 = rhymed_end_1[i]\n",
    "        c2 = rhymed_end_2[j]\n",
    "#         print(\"c1=\", c1, \"c2=\", c2)\n",
    "        consonant = Utils.getConsonant(c2)\n",
    "        if c1 != c2 and c1 != consonant and i > 1:\n",
    "            return False\n",
    "\n",
    "        j = j - 1\n",
    "\n",
    "    return True\n",
    "\n",
    "print(is_rhyme(\"блянина\", \"ссанина\"))\n",
    "print(is_rhyme(\"лиан\", \"виан\"))\n",
    "print(is_rhyme(\"говняшка\", \"неваляшка\"))\n",
    "print(is_rhyme(\"пидор\", \"залупа\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсим тексты оксимирона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Если у нас уже есть обработанные корпуса, можно их загрузить\n",
    "#style_corpus, style_corpus_soundex, style_corpus_soundex_list, style_corpus_pos_syllables, style_corpus_pos_syllables_list, style_corpus_syllables, style_corpus_syllables_list = load(\n",
    "#    \"style_corpus\", \"style_corpus_soundex\", \"style_corpus_soundex_list\", \"style_corpus_pos_syllables\", \"style_corpus_pos_syllables_list\", \"style_corpus_syllables\", \"style_corpus_syllables_list\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "style_corpus = []\n",
    "with open(STYLE_FILENAME) as f:\n",
    "    for line in f:\n",
    "        cleared_line = \" \".join(WORD_RE.findall(line))\n",
    "        if cleared_line:\n",
    "            style_corpus += [w for w in cleared_line.lower().split(\" \") if w and not w.isnumeric()]\n",
    "            style_corpus.append(\"/\")\n",
    "\n",
    "print(style_corpus[:100])\n",
    "\n",
    "style_corpus_soundex = {}\n",
    "style_corpus_soundex_list = []\n",
    "for w in style_corpus:\n",
    "    sdx = pos_soundex(w)\n",
    "    style_corpus_soundex_list.append(sdx)\n",
    "    style_corpus_soundex[sdx] = w\n",
    "\n",
    "print(style_corpus_soundex_list[:100])\n",
    "\n",
    "style_corpus_syllables = {}\n",
    "style_corpus_syllables_list = []\n",
    "for w in style_corpus:\n",
    "    sdx = syllables_soundex(w)\n",
    "    style_corpus_syllables_list.append(sdx)\n",
    "    style_corpus_syllables[sdx] = w\n",
    "    \n",
    "print(style_corpus_syllables_list[:100])\n",
    "\n",
    "style_corpus_pos_syllables = {}\n",
    "style_corpus_pos_syllables_list = []\n",
    "for w in style_corpus:\n",
    "    sdx = pos_syllables_soundex(w)\n",
    "    style_corpus_pos_syllables_list.append(sdx)\n",
    "    style_corpus_pos_syllables[sdx] = w\n",
    "    \n",
    "print(style_corpus_pos_syllables_list[:100])\n",
    "\n",
    "save(\n",
    "    style_corpus=style_corpus, \n",
    "    style_corpus_soundex=style_corpus_soundex,\n",
    "    style_corpus_soundex_list=style_corpus_soundex_list,\n",
    "    style_corpus_syllables=style_corpus_syllables,\n",
    "    style_corpus_syllables_list=style_corpus_syllables_list,\n",
    "    style_corpus_pos_syllables=style_corpus_pos_syllables,\n",
    "    style_corpus_pos_syllables_list=style_corpus_pos_syllables_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсим Хокинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hawking_content_corpus, hawking_content_corpus_soundex, hawking_content_corpus_accents, hawking_content_corpus_pos_syllables = load(\n",
    "#    \"hawking_content_corpus\", \"hawking_content_corpus_soundex\", \"hawking_content_corpus_accents\", \"hawking_content_corpus_pos_syllables\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hawking_content_corpus = []\n",
    "with open(HAWKING_CONTENT_FILENAME) as f:\n",
    "    for line in f:\n",
    "        cleared_line = \" \".join(WORD_RE.findall(line))\n",
    "        if cleared_line:\n",
    "            hawking_content_corpus += [w for w in cleared_line.lower().split(\" \") if w and len(w) > 1 and not w.isnumeric()]\n",
    "\n",
    "hawking_content_corpus = list(set(hawking_content_corpus))\n",
    "print(hawking_content_corpus[:100])\n",
    "hawking_content_corpus_soundex = {pos_soundex(w): w for w in hawking_content_corpus if w}\n",
    "print(list(hawking_content_corpus_soundex.keys())[:100])\n",
    "# hawking_content_corpus_accents = {accent_soundex(w): w for w in hawking_content_corpus if w}\n",
    "# print(list(hawking_content_corpus_accents.keys())[:100])\n",
    "hawking_content_corpus_syllables = {syllables_soundex(w): w for w in hawking_content_corpus if w}\n",
    "print(list(hawking_content_corpus_syllables.keys())[:100])\n",
    "hawking_content_corpus_pos_syllables = {pos_syllables_soundex(w): w for w in hawking_content_corpus if w}\n",
    "print(list(hawking_content_corpus_pos_syllables.keys())[:100])\n",
    "\n",
    "save(\n",
    "    hawking_content_corpus=hawking_content_corpus, \n",
    "    hawking_content_corpus_soundex=hawking_content_corpus_soundex,\n",
    "    #hawking_content_corpus_accents=hawking_content_corpus_accents,\n",
    "    hawking_content_corpus_syllables=hawking_content_corpus_syllables,\n",
    "    hawking_content_corpus_pos_syllables=hawking_content_corpus_pos_syllables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсим УК"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#uk_content_corpus, uk_content_corpus_soundex, uk_content_corpus_accents, uk_content_corpus_pos_syllables = load(\n",
    "#    \"uk_content_corpus\", \"uk_content_corpus_soundex\", \"uk_content_corpus_accents\", \"uk_content_corpus_pos_syllables\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UK_CONTENT_FILENAME = \"../uk.txt\"\n",
    "uk_content_corpus = []\n",
    "with open(UK_CONTENT_FILENAME) as f:\n",
    "    for line in f:\n",
    "        cleared_line = \" \".join(WORD_RE.findall(line))\n",
    "        if cleared_line:\n",
    "            uk_content_corpus += [w for w in cleared_line.lower().split(\" \") if w and len(w) > 1 and not w.isnumeric()]\n",
    "\n",
    "uk_content_corpus = list(set(uk_content_corpus))\n",
    "print(uk_content_corpus[:100])\n",
    "uk_content_corpus_soundex = {pos_soundex(w): w for w in uk_content_corpus if w}\n",
    "print(list(uk_content_corpus_soundex.keys())[:100])\n",
    "uk_content_corpus_syllables = {syllables_soundex(w): w for w in uk_content_corpus if w}\n",
    "print(list(uk_content_corpus_syllables.keys())[:100])\n",
    "uk_content_corpus_pos_syllables = {pos_syllables_soundex(w): w for w in uk_content_corpus if w}\n",
    "print(list(uk_content_corpus_pos_syllables.keys())[:100])\n",
    "\n",
    "\n",
    "save(\n",
    "    uk_content_corpus=uk_content_corpus, \n",
    "    uk_content_corpus_soundex=uk_content_corpus_soundex,\n",
    "    uk_content_corpus_syllables=uk_content_corpus_syllables,\n",
    "    uk_content_corpus_pos_syllables=uk_content_corpus_pos_syllables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пошла нейросеть\n",
    "\n",
    "Тут всё слизано из examples к самому Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \" \".join(style_corpus_pos_syllables_list)\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 25\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 50):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y,\n",
    "              batch_size=128,\n",
    "              epochs=2)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = '' \n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(300):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('oxxxy_pos_syllables_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательный метод для генерации слов сеточкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_rnn(count):\n",
    "    generated = \"\"\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "\n",
    "    for i in range(count):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Самый простой вариант: генерим сетью и выводим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = uk_content_corpus_soundex\n",
    "generated = generate_rnn(2000).split(\" \")\n",
    "print(generated)\n",
    "for term in generated:\n",
    "    if term == \"/\":\n",
    "        print(term)\n",
    "        continue\n",
    "        \n",
    "    guessed_words = {}\n",
    "    for idx, word in content.items():\n",
    "        lev_dist = levenshtein_distance(idx, term)\n",
    "        if len(word) > 1 and lev_dist <= 1:\n",
    "            guessed_words[word] = lev_dist\n",
    "           \n",
    "    if not guessed_words:\n",
    "        for idx, word in style_corpus_soundex.items():\n",
    "            lev_dist = levenshtein_distance(idx, term)\n",
    "            if lev_dist <= 1:\n",
    "                guessed_words[word] = lev_dist\n",
    "    \n",
    "    print(sorted(guessed_words, key=guessed_words.get)[0] if guessed_words else \"\", end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариант посложнее: рифмуем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rhyme_end(word):\n",
    "    word_num_syllables, word_accent_syllable = accents.getAccentsAndSyllablesWord(word)\n",
    "    word_accented_syll_offset = word_num_syllables - word_accent_syllable\n",
    "    return RhymesAndRitms.getRhymedEnd(word, word_accented_syll_offset)\n",
    "    \n",
    "def check_rhyme_end_with_word(rhymed_end_1, word):\n",
    "    word_num_syllables, word_accent_syllable = accents.getAccentsAndSyllablesWord(word)\n",
    "    word_accented_syll_offset = word_num_syllables - word_accent_syllable\n",
    "    rhymed_end_2 = RhymesAndRitms.getRhymedEnd(word, word_accented_syll_offset)\n",
    "    \n",
    "    if not rhymed_end_1 or not rhymed_end_2:\n",
    "        return False\n",
    "    \n",
    "    j = len(rhymed_end_2) - 1\n",
    "    for i in range(len(rhymed_end_1) - 1, -1, -1):\n",
    "        if j < 0:\n",
    "            return True\n",
    "\n",
    "        c1 = rhymed_end_1[i]\n",
    "        c2 = rhymed_end_2[j]\n",
    "        consonant = Utils.getConsonant(c2)\n",
    "#         print(\"i=\", i, \"c1=\", c1, \"c2=\", c2, \"cons=\", consonant)\n",
    "        if c1 != c2 and c1 != consonant and i > 1:\n",
    "            return False\n",
    "\n",
    "        j = j - 1\n",
    "\n",
    "    return True\n",
    "\n",
    "rhymed_end = get_rhyme_end(\"блядина\")\n",
    "print(rhymed_end)\n",
    "print(check_rhyme_end_with_word(rhymed_end, \"ссанина\"))\n",
    "print(check_rhyme_end_with_word(rhymed_end, \"ебанина\"))\n",
    "print(check_rhyme_end_with_word(rhymed_end, \"говно\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content = hawking_content_corpus_soundex\n",
    "cache = set()\n",
    "lines = []\n",
    "rhymed_ends = defaultdict(list)\n",
    "iteration = 0\n",
    "while iteration < 100:\n",
    "    iteration += 1\n",
    "    lines = generate_rnn(10000).split(\"/\")\n",
    "    print(\"generated\", len(lines))\n",
    "    for line in lines:\n",
    "        terms = [t for t in line.split(\" \") if t]\n",
    "        if not terms:\n",
    "            continue\n",
    "        \n",
    "        generated_line = []\n",
    "        for term in terms[:-1]:\n",
    "            guessed_words = {}\n",
    "            for idx, word in content.items():\n",
    "                lev_dist = levenshtein_distance(idx, term)\n",
    "                if len(word) > 1 and lev_dist <= 1:\n",
    "                    guessed_words[word] = lev_dist\n",
    "            \n",
    "            if not guessed_words:\n",
    "                for idx, word in style_corpus_soundex.items():\n",
    "                    lev_dist = levenshtein_distance(idx, term)\n",
    "                    if lev_dist <= 1:\n",
    "                        guessed_words[word] = lev_dist\n",
    "                        \n",
    "            if guessed_words:\n",
    "                generated_line.append(sorted(guessed_words, key=guessed_words.get)[0])\n",
    "        \n",
    "        last_words = []\n",
    "        last_term = terms[-1]\n",
    "        for idx, word in content.items():\n",
    "            lev_dist = levenshtein_distance(idx, last_term)\n",
    "            if len(word) > 1 and lev_dist <= 1:\n",
    "                last_words.append(word)\n",
    "                \n",
    "        generated_line_str = \" \".join(generated_line)\n",
    "        for last_word in last_words:\n",
    "            if last_word in rhymed_ends:\n",
    "                continue\n",
    "            \n",
    "            rhymed_end = get_rhyme_end(last_word)\n",
    "            for word in rhymed_ends.keys():\n",
    "                if check_rhyme_end_with_word(rhymed_end, word):\n",
    "                    rhymed_ends[word].append(\"{} {}\".format(generated_line_str, last_word))\n",
    "                    break\n",
    "            else:\n",
    "                rhymed_ends[last_word].append(\"{} {}\".format(generated_line_str, last_word))\n",
    "            \n",
    "    print(\"Iteration {} completed\".format(iteration))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for values in rhymed_ends.values():\n",
    "    if len(values) > 3:\n",
    "        print(\"-\" * 30)\n",
    "        for value in values:\n",
    "            print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
